{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Case Study\n",
    "\n",
    "## Map Area and Source\n",
    "\n",
    "The map area covers Singapore, SG. I live here.\n",
    "\n",
    "Map Source: [Mapzen Metro Extract](https://mapzen.com/data/metro-extracts/metro/singapore/)\n",
    "\n",
    "## Problems Encountered In the Map\n",
    "A first-pass ETL of `singapore.osm` into a SQLite3 database via `data.py`, several problems were identified and addressed in order:\n",
    "- *Lorong*, the local equivalent of *street* or *avenue* has been expressed inconsistently - some abbreviated (i.e. `Lor`), some in full (i.e. '`Lorong`')\n",
    "- *Lorong*s not in consistent order (e.g. some `Toa Payoh Lorong 1`, some `Lorong 1 Geylang`)\n",
    "- Invalid postcodes (e.g. `#B1-42`, `Johor Bahru`, `<different>`)\n",
    "- Improperly formed postcodes (e.g. `S 278989`, `Singapore 408564`)\n",
    "- Non-local 5-digit postcodes (e.g. `18953`)\n",
    "\n",
    "### Detecting and Fixing Lorongs\n",
    "\n",
    "The preferred way of referring *Lorong*-style streets is having *Lorong* first, as opposed to regular street names in English (e.g Pickering Street). A number or name follows *Lorong*:\n",
    "- Lorong 1 Geylang\n",
    "- Lorong Chuan\n",
    "- Lorong Ah Soo\n",
    "\n",
    "Inconsistent abbreviations were found via a regex audit:\n",
    "```python\n",
    "def audit_lorong_type(lorong_types, street_name):\n",
    "    \"\"\" Process addr:street tag\n",
    "    Args:\n",
    "        lorong_types - Dictionary of keys that are Lorong-street types\n",
    "        street_name - Street name; 'v' attribute of addr:street\n",
    "    \"\"\"\n",
    "    lorong_type_re = re.compile(r'(\\blor\\b)|(\\blorong\\b)', re.IGNORECASE)\n",
    "\n",
    "    match_result = lorong_type_re.search(street_name)\n",
    "    if match_result:\n",
    "        lorong_type = match_result.group()\n",
    "        lorong_types[lorong_type].add(street_name)\n",
    "```\n",
    "and fixed via a simple search/replace function, shown together with the subsequent fix.\n",
    "\n",
    "__*Lorongs* First!__\n",
    "\n",
    "If, for example '*Lorong* 1 Geylang' is inverted, i.e 'Geylang *Lorong* 1' - this is still perfectly understandable, however, examples like 'Chuan *Lorong*' is unheard of in local vernacular. Hence, having *Lorong* in front is consistent.\n",
    "\n",
    "One caveat: if a *Lorong*-style street name begins with a digit, e.g. '2 Lorong Napiri', it would be better practice to ignore it, as it does not refer to a street, so swapping 'Lorong Napiri 2' will not make sense. The code below takes care of the street name cleanup.\n",
    "```python\n",
    "def clean_lorong(sname):\n",
    "    \"\"\" Cleans lorong-style street names\n",
    "    Args:\n",
    "        sname - Street name; 'v' attribute of addr:street\n",
    "    Returns:\n",
    "        Cleaned street name\n",
    "    \"\"\"\n",
    "    verbose = True\n",
    "\n",
    "    # first find and replace Lor with Lorong\n",
    "    result = re.sub(r'\\bLor\\b', 'Lorong', sname)\n",
    "    # if sname != result: print(\"original: {}, cleaned: {}\".format(sname, result))\n",
    "\n",
    "    # then swap order of Lorong if applicable\n",
    "    idx_l = result.find('Lorong')\n",
    "    # if street name does not begin with lorong & a digit, swap lorong\n",
    "    if (idx_l > 0) and (result[0] not in set('123456890')):\n",
    "        newname = result[idx_l:]+ \" \" + result[:idx_l]\n",
    "        if verbose: print(\"swap: {} -> {}\".format(result, newname))\n",
    "        return newname\n",
    "    else:\n",
    "        return result\n",
    "```\n",
    "\n",
    "### Detecting and Fixing Postcodes\n",
    "Singapore postal codes are strictly 6 digits in length, with the first 2 digits denoting the postal sector. A simple regular expression can be used to check for non-compliance:\n",
    "```python\n",
    "def audit_postcode_type(postcode_types, postcode):\n",
    "    \"\"\" Process addr:postcode tag\n",
    "    Args:\n",
    "        postcode_types - Dictionary of keys not compliant to Singapore postcode format\n",
    "        postcode - Postcode; 'v' attribute of addr:postcode\n",
    "    \"\"\"\n",
    "    postcode_type_re = re.compile(r'^\\d{6}$')\n",
    "\n",
    "    match_result = postcode_type_re.search(postcode)\n",
    "    if not match_result:\n",
    "        postcode_types[postcode].add(postcode)\n",
    "```\n",
    "The non-compliant postal codes fell into 3 categories - either invalid, malformed or non-Singapore codes. The Mapzen metro extract boundaries extend past Singapore's borders, into both neighbouring Malaysia as well as Indonesia. This accounts for the number of 5-digit postal codes in the data set, which are simply ignored in this analysis.\n",
    "\n",
    "Malformed codes can be corrected by stripping all other characters and returning the 6-digit postal code.\n",
    "\n",
    "To resolve the other invalid codes it was possible to either update the postal code to a valid code that does not exist<sup>[1]</sup> (i.e. `000000`), or simply drop the tag. The first option of updating the postal code was chosen, to preserve the tag counts/statistics for later analysis.\n",
    "```python\n",
    "def clean_postcode(postcode):\n",
    "    \"\"\" Cleans postcode\n",
    "    Args:\n",
    "        postcode - Postcode; 'v' attribute of addr:postcode\n",
    "    Returns:\n",
    "        Cleaned postcode\n",
    "    \"\"\"\n",
    "    verbose = True\n",
    "    pattern = re.compile(r'^\\d{5,6}$') # valid codes are 5 or 6-digits\n",
    "\n",
    "    match_result = pattern.search(postcode)\n",
    "    if not match_result: # process non-compliant codes\n",
    "        result = re.sub(r'[^\\d]', '', postcode) # clean up malformed codes\n",
    "    else: # do not process compliant codes\n",
    "        return postcode\n",
    "\n",
    "    if len(result) != 6: # if not compliant after clean up, set default postcode\n",
    "        result = \"000000\"\n",
    "\n",
    "    if verbose: print(\"original: {}, cleaned: {}\".format(postcode, result))\n",
    "    return result\n",
    "```\n",
    "\n",
    "## Overview of the Data & Additional Queries\n",
    "An overview of the data sources is provided in this section. The source OSM XML file was downloaded on **26 Aug 2017** from a standard Mapzen Metro Extract.\n",
    "### Map Data Quality\n",
    "The overall data quality with regards to street names is remarkably consistent and clean. This could be due to the fact that Singapore has strict guidelines on how streets are named<sup>[2]</sup>.\n",
    "### Database and Source Files\n",
    "\n",
    "File | Size | Type\n",
    "--- | --- | ---\n",
    "singapore.osm | 329M | OpenStreetMap XML\n",
    "singapore.sqlite | 230M | SQLite3 DB\n",
    "nodes.csv | 119M | Nodes CSV File\n",
    "nodes_tags.csv | 4.9M | Node Tags CSV File\n",
    "ways.csv | 14M | Ways CSV File\n",
    "ways_nodes.csv | 44M | Way Nodes CSV File\n",
    "ways_tags.csv | 22M | Way Node Tags CSV File\n",
    "\n",
    "### Nodes & Ways\n",
    "The number of nodes extracted from `singapore.osm` are as follows:\n",
    "```sql\n",
    "sqlite> select count(*) as n from nodes;\n",
    "n                   \n",
    "--------------------\n",
    "1513629             \n",
    "```\n",
    "The number of ways extracted from `singapore.osm` are as follows:\n",
    "```sql\n",
    "sqlite> select count(*) as n from ways;\n",
    "n                   \n",
    "--------------------\n",
    "237163              \n",
    "```\n",
    "### Number of Unique Users\n",
    "```sql\n",
    "sqlite> select count(*) from \n",
    "            (select u.user from \n",
    "                (select user from nodes union all select user from ways) as u \n",
    "            group by u.user) as c;\n",
    "count(*)                 \n",
    "-------------------------\n",
    "2144                     \n",
    "\n",
    "```\n",
    "### Changesets\n",
    "A changeset can be open for up to 24 hours, and have maximum of 10,000 entries. The 10-largest changesets are:\n",
    "```sql\n",
    "sqlite> select u.changeset, count(*) as n from\n",
    "   ...> (select changeset from nodes union all select changeset from ways) as u\n",
    "   ...> group by u.changeset order by n desc limit 10;\n",
    "changeset             n         \n",
    "--------------------  ----------\n",
    "35657677              9875      \n",
    "29647823              9870      \n",
    "25268928              9009      \n",
    "40738823              8938      \n",
    "15807798              8717      \n",
    "19508643              8682      \n",
    "27022028              7786      \n",
    "24098829              7544      \n",
    "19934418              7039      \n",
    "40236725              5949      \n",
    "```\n",
    "The size of changesets probably reveals that the users of these changesets are bots.\n",
    "### Which users made the 10-largest changesets?\n",
    "```sql\n",
    "sqlite> select j.user from \n",
    "            (select user, changeset from nodes \n",
    "            union all \n",
    "            select user, changeset from ways) as j \n",
    "        join \n",
    "            (select u.changeset, count(*) as n from \n",
    "                (select changeset from nodes \n",
    "                union all \n",
    "                select changeset from ways) as u \n",
    "             group by u.changeset order by n desc limit 10) as k\n",
    "        on j.changeset = k.changeset \n",
    "        group by j.user;\n",
    "user                \n",
    "--------------------\n",
    "JaLooNz             \n",
    "mdk                 \n",
    "```\n",
    "The top 10 largest changesets were made by only 2 users! This lends more evidence that the these users contributing to the largest changesets are bots. Another check for largest contributors corroborates this as well:\n",
    "```sql\n",
    "sqlite> select u.user, count(*) as n from \n",
    "            (select user from nodes union all select user from ways) as u \n",
    "        group by u.user order by n desc limit 10;\n",
    "user                  n         \n",
    "--------------------  ----------\n",
    "JaLooNz               405158    \n",
    "berjaya               117460    \n",
    "rene78                77593     \n",
    "cboothroyd            72280     \n",
    "lmum                  50780     \n",
    "kingrollo             39068     \n",
    "Luis36995             38823     \n",
    "ridixcr               38240     \n",
    "Sihabul Milah         37160     \n",
    "calfarome             32946     \n",
    "```\n",
    "\n",
    "## Suggestions for Improvement\n",
    "OSM provides GPS longitude and latitude data - but does it also record altitude data? OSM documentation<sup>[3]</sup> on this shows that it could potentially be stored in `ele=` tags. A search on the full Singapore dataset reveals that there are very few such tags compared to the circa 1.5 million nodes in the dataset:\n",
    "```sql\n",
    "sqlite> select count(*) from (select * from nodes_tags union all select * from ways_tags) as u where key = 'ele';\n",
    "54\n",
    "```\n",
    "The OSM documentation also reveals the difficulty in recording altitude data:\n",
    "- Altitude information from consumer-grade GPS devices is often not accurate enough\n",
    "- Lack of definitive reference altitudes in Singapore (and around the world) for other readings to compare accurately from\n",
    "- Pressure/barometric-based altimeters are unstable (dependent on temperature and other factors) requiring frequent calibration\n",
    "- Many reference systems/elevation models of altitude measurement (e.g. Nullebene, Geoid, Referenzellipsoid) requires a significant amount of skill and knowledge from the contributor\n",
    "- Measuring depth is yet another complication \n",
    "\n",
    "A possible solution to this, is to involve big data. While GPS readings from a single contributor may not offer the desired accuracy, GPS tracks from multiple contributors could be taken instead. Building 3-dimensional location heatmaps of a location to achieve decent accuracy. This new data would need to be processed with big data methods because:\n",
    "- High data volume (fine-grained crowd-sourced data)\n",
    "- High data veracity (expect noise, bias in the contributor data)\n",
    "- Moderate data variety (Elevation data could be in the form of GPS, barometric readings, different systems, different formats; raw data could even come from pictures/videos with GPS tags, etc)\n",
    "\n",
    "Further down the data pipeline, more advanced statistical or machine-learning methods would need to be applied to clean and analyse the data.\n",
    "\n",
    "As availability and accuracy of the resulting altitude data is dependent on contributor data, the limitations on this approach would be obvious - areas with fewer contributors will have less reliable altitude information. In the future, devices or methods to obtain accurate elevation data may be available and reliance on contributer data would be reduced. \n",
    "\n",
    "However, I still think it is a good starting point sufficient for many uses - sufficiently accurate elevation data for hiking, biking, driving.\n",
    "\n",
    "## Additional Ideas\n",
    "\n",
    "### Most Common Speed Limit on Asphalt Roads\n",
    "The official speed limit in Singapore is 50 kilometers per hour on most roads. The query returns 60 instead. Analysing the reasons may require understanding the nature of the roads (e.g. geographical features, location) or source of `maxspeed`.\n",
    "```sql\n",
    "sqlite> select key, value, count(*) as n from ways_tags where id in \n",
    "            (select id from ways_tags where value = 'asphalt') and key = 'maxspeed' \n",
    "        group by key, value order by n desc limit 5;\n",
    "key                        value       n         \n",
    "-------------------------  ----------  ----------\n",
    "maxspeed                   60          3136      \n",
    "maxspeed                   50          1875      \n",
    "maxspeed                   70          1336      \n",
    "maxspeed                   90          668       \n",
    "maxspeed                   40          224       \n",
    "```\n",
    "### Common Cafes\n",
    "The most common cafe - Starbucks, appears in multiple rows below, due to the different naming conventions of the company. Cleaning data from any `name=` fields will be daunting, as there is considerable variety of correct/valid names with little constraints. Improving the query may require tools that SQL may find challenging.  \n",
    "```sql\n",
    "sqlite> select value, count(*) as n from nodes_tags where id in \n",
    "            (select id from nodes_tags where value = 'cafe') and key = 'name' \n",
    "        group by value order by n desc limit 10;\n",
    "value                      n         \n",
    "-------------------------  ----------\n",
    "Starbucks                  58        \n",
    "The Coffee Bean & Tea Lea  13        \n",
    "Toast Box                  8         \n",
    "Starbucks Coffee           6         \n",
    "Coffee Bean & Tea Leaf     4         \n",
    "Coffee Shop                4         \n",
    "The Coffee Bean and Tea L  4         \n",
    "Killiney Kopitiam          3         \n",
    "Koi Cafe                   3         \n",
    "Lola's Cafe                3         \n",
    "```\n",
    "### Unicode Data\n",
    "Some unicode tags in Chinese language were discovered, which mostly consisted of road names. This raises an interesting point - these unicode data will have to be handled separately in the data audit and cleaning process, because Singapore is a multi-cultural/multi-language society. Inclusions of other language data will increase in future.\n",
    "```sql\n",
    "sqlite> select u.value, count(*) as n from \n",
    "            (select key, value from nodes_tags union all \n",
    "                select key, value from ways_tags) as u \n",
    "        where key = 'zh' group by u.value order by n desc limit 10;\n",
    "value                      n         \n",
    "-------------------------  ----------\n",
    "南北大道                       64        \n",
    "第二通道高速公路                   43        \n",
    "新柔长堤                       35        \n",
    "麦士威路                       15        \n",
    "克罗士街上段                     13        \n",
    "尼路                         11        \n",
    "奎因街                        9         \n",
    "桥南路                        9         \n",
    "迪沙鲁路                       7         \n",
    "笨珍路                        6         \n",
    "```\n",
    "## Conclusion\n",
    "This report covered the most pertinent problems encountered when auditing and cleaning the data set and provided a brief overview of the map data including users, nodes and ways as well as some interesting queries.\n",
    "\n",
    "Street-level data was generally clean but at the same time, scope for further improvements can be made in the areas highlighted from the preceding section.\n",
    "\n",
    "[1]:https://www.ura.gov.sg/realEstateIIWeb/resources/misc/list_of_postal_districts.htm\n",
    "[2]:https://www.ura.gov.sg/uol/guidelines/development-control/street-naming\n",
    "[3]:http://wiki.openstreetmap.org/wiki/Altitude"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
